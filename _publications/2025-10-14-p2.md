---
title: "Enhancing Code Optimization in LLMs with Dual Encoder Architecture for Syntax and Semantic Refinement"
collection: publications
category: conferences
permalink: /publication/2025-09-23-pInterpretable-Vulnerability-Detection-in-LLMs-A-BERT-Based-Approach-with-SHAP-Explanations
excerpt: 'LLMs'
authors: 'Nouman Ahmad*, Changsheng Zhang'

date: 2025-10-13
venue: '2025 International Conference on Artificial Intelligence and Digital Ethics (ICAIDE)'
volume:
issue:
pages: '406-409'
doi: '10.1109/ICAIDE65466.2025.11189637'
codeurl:
slidesurl:
paperurl: 'https://ieeexplore.ieee.org/document/11189637'
citation: '@INPROCEEDINGS{11189637,
  author={Ahmad, Nouman and Zhang, Changsheng},
  booktitle={2025 International Conference on Artificial Intelligence and Digital Ethics (ICAIDE)}, 
  title={Enhancing Code Optimization in LLMs with Dual Encoder Architecture for Syntax and Semantic Refinement}, 
  year={2025},
  volume={},
  number={},
  pages={406-409},
  keywords={Measurement;Adaptation models;Codes;Pipelines;Semantics;Retrieval augmented generation;Reinforcement learning;Syntactics;Routing;Optimization;component;BLEU Score, Code Optimization, CodeBERT, CodeLlama, Syntax Correction},
  doi={10.1109/ICAIDE65466.2025.11189637}}
'
---

Despite their impressive efficiency in code generation, large language models (LLMs) frequently perform poorly on code optimization objectives like lowering algorithmic complexity or enhancing code quality. In order to improve code optimization capabilities in LLMs, we present a unique dual-encoder architecture in this study. After the encoder stage of the DeepSeek-Coder 33B model, we specifically incorporate two specialized encoders: (1) CodeBERT-base for syntactic analysis and correction, and (2) CodeLlama-7b-Optimize for semantic-level code modifications. In order to provide cleaner, more effective code outputs, our design divides the tasks of semantic optimization and syntax inspection. According to empirical tests, our method maintains a low inference latency of 170 ms per sample, obtains a BLEU score of 88, and corrects grammar with 94% correctness. These outcomes demonstrate how well specialized encoder modules work to get beyond the present drawbacks of LLMs in code optimization.
